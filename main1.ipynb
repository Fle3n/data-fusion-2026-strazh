{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ":root {\n",
    "  --bg1: #0f172a;\n",
    "  --bg2: #1e293b;\n",
    "  --line: #cbd5e1;\n",
    "  --accent: #16a34a;\n",
    "}\n",
    ".hero {\n",
    "  background: linear-gradient(135deg, var(--bg1), var(--bg2));\n",
    "  color: #e2e8f0;\n",
    "  border-radius: 18px;\n",
    "  padding: 22px 24px;\n",
    "  margin-bottom: 12px;\n",
    "  box-shadow: 0 8px 24px rgba(2, 6, 23, 0.35);\n",
    "}\n",
    ".hero h1 {\n",
    "  margin: 0 0 8px 0;\n",
    "  font-size: 32px;\n",
    "}\n",
    ".hero p {\n",
    "  margin: 6px 0;\n",
    "  line-height: 1.45;\n",
    "}\n",
    ".pill {\n",
    "  display: inline-block;\n",
    "  margin: 4px 6px 0 0;\n",
    "  padding: 4px 10px;\n",
    "  border: 1px solid rgba(148, 163, 184, 0.5);\n",
    "  border-radius: 999px;\n",
    "  font-size: 12px;\n",
    "}\n",
    ".block {\n",
    "  border-left: 4px solid var(--accent);\n",
    "  padding: 12px 14px;\n",
    "  border-radius: 8px;\n",
    "  margin: 10px 0;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"hero\">\n",
    "  <h1>DataFusion TASK1 • main-full.ipynb</h1>\n",
    "  <p>Полный ноутбук антифрод-пайплайна: временные признаки, sequence-фичи, category priors, ансамбль CatBoost и подготовка сабмита.</p>\n",
    "  <div>\n",
    "    <span class=\"pill\">Метрика: PR-AUC</span>\n",
    "    <span class=\"pill\">Целевая задача: red vs all</span>\n",
    "    <span class=\"pill\">Режим: time-aware</span>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div class=\"block\">\n",
    "  <b>Ключевая идея:</b> объединяем несколько моделей с разной специализацией:\n",
    "  глобальный red-классификатор, recent-модель, «suspicious vs green» и «red vs yellow».\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оглавление\n",
    "\n",
    "1. Импорты и глобальный конфиг.\n",
    "2. Описание колонок и загрузка разметки.\n",
    "3. Feature engineering на полной временной истории.\n",
    "4. Сборка train/test фичей.\n",
    "5. Category priors из train.\n",
    "6. Подготовка матриц для CatBoost.\n",
    "7. Обучение 4 моделей и блендинг.\n",
    "8. Переобучение на полном train и формирование `submission.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Импорты и параметры запуска\n",
    "Проверяем пути, флаги и базовые тайм-границы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c06c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: E:\\AI\\DataFusion\\TASK1\\data\n",
      "CACHE_DIR: E:\\AI\\DataFusion\\TASK1\\cache\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Блок 1. Импорты, базовые настройки среды и глобальные флаги.\n",
    "# ------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pl.Config.set_tbl_rows(12)\n",
    "pl.Config.set_tbl_cols(200)\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "CACHE_DIR = Path(\"cache\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# sampling negatives (green) from train period\n",
    "NEG_SAMPLE_MOD_RECENT = 10  # from 2025-04-01\n",
    "NEG_SAMPLE_MOD_OLD = 30     # before 2025-04-01\n",
    "NEG_SAMPLE_BORDER_STR = \"2025-04-01 00:00:00\"\n",
    "\n",
    "# holdout for local validation\n",
    "VAL_START = pd.Timestamp(\"2025-05-01\")\n",
    "RECENT_BORDER = pd.Timestamp(\"2025-02-01\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "FORCE_REBUILD_FEATURES = True\n",
    "FORCE_REBUILD_PRIORS = False\n",
    "ADD_CATEGORY_PRIORS = True\n",
    "USE_GPU = True\n",
    "RETRAIN_ON_FULL = True\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR.resolve())\n",
    "print(\"CACHE_DIR:\", CACHE_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Схема признаков\n",
    "Фиксируем список сырьевых и итоговых фичей для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64749fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: (87514, 3)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Блок 2. Списки колонок, фичей и служебных полей.\n",
    "# ------------------------------------------------------------\n",
    "BASE_COLS = [\n",
    "    \"customer_id\", \"event_id\", \"event_dttm\", \"event_type_nm\", \"event_desc\",\n",
    "    \"channel_indicator_type\", \"channel_indicator_sub_type\", \"operaton_amt\", \"currency_iso_cd\",\n",
    "    \"mcc_code\", \"pos_cd\", \"timezone\", \"session_id\", \"operating_system_type\",\n",
    "    \"battery\", \"device_system_version\", \"screen_size\", \"developer_tools\",\n",
    "    \"phone_voip_call_state\", \"web_rdp_connection\", \"compromised\"\n",
    "]\n",
    "\n",
    "FINAL_FEATURE_COLS = [\n",
    "    # raw (mostly categorical)\n",
    "    \"customer_id\", \"event_type_nm\", \"event_desc\", \"channel_indicator_type\",\n",
    "    \"channel_indicator_sub_type\", \"currency_iso_cd\", \"mcc_code_i\", \"pos_cd\", \"timezone\",\n",
    "    \"operating_system_type\", \"phone_voip_call_state\", \"web_rdp_connection\",\n",
    "    \"developer_tools_i\", \"compromised_i\",\n",
    "    # event numeric\n",
    "    \"amt\", \"amt_log_abs\", \"amt_is_negative\", \"hour\", \"weekday\", \"day\", \"month\",\n",
    "    \"is_weekend\", \"event_day_number\", \"battery_pct\", \"os_ver_major\", \"screen_w\",\n",
    "    \"screen_h\", \"screen_pixels\", \"screen_ratio\", \"session_id\",\n",
    "    # sequence\n",
    "    \"cust_prev_events\", \"cust_prev_amt_mean\", \"cust_prev_amt_std\", \"sec_since_prev_event\",\n",
    "    \"amt_delta_prev\", \"cnt_prev_same_type\", \"cnt_prev_same_desc\", \"cnt_prev_same_mcc\",\n",
    "    \"cnt_prev_same_subtype\", \"cnt_prev_same_session\", \"sec_since_prev_same_type\",\n",
    "    \"sec_since_prev_same_desc\", \"events_before_today\",\n",
    "]\n",
    "\n",
    "CAT_COLS = [\n",
    "    \"customer_id\", \"event_type_nm\", \"event_desc\", \"channel_indicator_type\",\n",
    "    \"channel_indicator_sub_type\", \"currency_iso_cd\", \"mcc_code_i\", \"pos_cd\",\n",
    "    \"timezone\", \"operating_system_type\", \"phone_voip_call_state\", \"web_rdp_connection\",\n",
    "    \"developer_tools_i\", \"compromised_i\",\n",
    "]\n",
    "\n",
    "META_COLS = [\"event_id\", \"period\", \"event_ts\", \"is_train_sample\", \"is_test\", \"train_target_raw\", \"target_bin\"]\n",
    "\n",
    "labels_lf = pl.scan_parquet(DATA_DIR / \"train_labels.parquet\")\n",
    "labels_df = pl.read_parquet(DATA_DIR / \"train_labels.parquet\")\n",
    "print(\"Labels:\", labels_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Построение временных и последовательных признаков\n",
    "В этом блоке важно, что все history-фичи строятся только по прошлым событиям клиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962c7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Блок 3. Feature engineering: объединение периодов, семплинг\n",
    "# зеленого класса, временные/поведенческие признаки и target_bin.\n",
    "# ------------------------------------------------------------\n",
    "def _period_frames_for_part(part_id: int) -> pl.LazyFrame:\n",
    "    custs_lf = (\n",
    "        pl.scan_parquet(DATA_DIR / f\"pretrain_part_{part_id}.parquet\")\n",
    "        .select(\"customer_id\")\n",
    "        .unique()\n",
    "    )\n",
    "\n",
    "    pretrain_lf = (\n",
    "        pl.scan_parquet(DATA_DIR / f\"pretrain_part_{part_id}.parquet\")\n",
    "        .select(BASE_COLS)\n",
    "        .with_columns(pl.lit(\"pretrain\").alias(\"period\"))\n",
    "    )\n",
    "    train_lf = (\n",
    "        pl.scan_parquet(DATA_DIR / f\"train_part_{part_id}.parquet\")\n",
    "        .select(BASE_COLS)\n",
    "        .with_columns(pl.lit(\"train\").alias(\"period\"))\n",
    "    )\n",
    "    pretest_lf = (\n",
    "        pl.scan_parquet(DATA_DIR / \"pretest.parquet\")\n",
    "        .select(BASE_COLS)\n",
    "        .join(custs_lf, on=\"customer_id\", how=\"inner\")\n",
    "        .with_columns(pl.lit(\"pretest\").alias(\"period\"))\n",
    "    )\n",
    "    test_lf = (\n",
    "        pl.scan_parquet(DATA_DIR / \"test.parquet\")\n",
    "        .select(BASE_COLS)\n",
    "        .join(custs_lf, on=\"customer_id\", how=\"inner\")\n",
    "        .with_columns(pl.lit(\"test\").alias(\"period\"))\n",
    "    )\n",
    "\n",
    "    return pl.concat([pretrain_lf, train_lf, pretest_lf, test_lf], how=\"vertical_relaxed\")\n",
    "\n",
    "\n",
    "def build_features_for_part(part_id: int, force: bool = False) -> Path:\n",
    "    out_path = CACHE_DIR / f\"features_part_{part_id}.parquet\"\n",
    "    if out_path.exists() and (not force):\n",
    "        print(f\"[part {part_id}] use cache -> {out_path.name}\")\n",
    "        return out_path\n",
    "\n",
    "    print(f\"[part {part_id}] building features...\")\n",
    "    lf = _period_frames_for_part(part_id)\n",
    "\n",
    "    # parse and normalize to numeric-only feature space for large-scale windows\n",
    "    lf = (\n",
    "        lf.with_columns([\n",
    "            pl.col(\"event_dttm\").str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\", strict=False).alias(\"event_ts\"),\n",
    "            pl.col(\"operaton_amt\").cast(pl.Float64).alias(\"amt\"),\n",
    "            pl.col(\"session_id\").cast(pl.Int64, strict=False).fill_null(-1).alias(\"session_id\"),\n",
    "\n",
    "            pl.col(\"event_type_nm\").cast(pl.Int32, strict=False).fill_null(-1).alias(\"event_type_nm\"),\n",
    "            pl.col(\"event_desc\").cast(pl.Int32, strict=False).fill_null(-1).alias(\"event_desc\"),\n",
    "            pl.col(\"channel_indicator_type\").cast(pl.Int16, strict=False).fill_null(-1).alias(\"channel_indicator_type\"),\n",
    "            pl.col(\"channel_indicator_sub_type\").cast(pl.Int16, strict=False).fill_null(-1).alias(\"channel_indicator_sub_type\"),\n",
    "            pl.col(\"currency_iso_cd\").cast(pl.Int16, strict=False).fill_null(-1).alias(\"currency_iso_cd\"),\n",
    "            pl.col(\"pos_cd\").cast(pl.Int16, strict=False).fill_null(-1).alias(\"pos_cd\"),\n",
    "            pl.col(\"timezone\").cast(pl.Int32, strict=False).fill_null(-1).alias(\"timezone\"),\n",
    "            pl.col(\"operating_system_type\").cast(pl.Int16, strict=False).fill_null(-1).alias(\"operating_system_type\"),\n",
    "            pl.col(\"phone_voip_call_state\").cast(pl.Int8, strict=False).fill_null(-1).alias(\"phone_voip_call_state\"),\n",
    "            pl.col(\"web_rdp_connection\").cast(pl.Int8, strict=False).fill_null(-1).alias(\"web_rdp_connection\"),\n",
    "\n",
    "            pl.col(\"mcc_code\").cast(pl.Int32, strict=False).fill_null(-1).alias(\"mcc_code_i\"),\n",
    "            pl.col(\"battery\").str.extract(r\"(\\\\d{1,3})\", 1).cast(pl.Int16, strict=False).fill_null(-1).alias(\"battery_pct\"),\n",
    "            pl.col(\"device_system_version\").str.extract(r\"^(\\\\d+)\", 1).cast(pl.Int16, strict=False).fill_null(-1).alias(\"os_ver_major\"),\n",
    "            pl.col(\"screen_size\").str.extract(r\"^(\\\\d+)\", 1).cast(pl.Int16, strict=False).fill_null(-1).alias(\"screen_w\"),\n",
    "            pl.col(\"screen_size\").str.extract(r\"x(\\\\d+)$\", 1).cast(pl.Int16, strict=False).fill_null(-1).alias(\"screen_h\"),\n",
    "            pl.col(\"developer_tools\").cast(pl.Int8, strict=False).fill_null(-1).alias(\"developer_tools_i\"),\n",
    "            pl.col(\"compromised\").cast(pl.Int8, strict=False).fill_null(-1).alias(\"compromised_i\"),\n",
    "        ])\n",
    "        .drop([\"event_dttm\", \"operaton_amt\", \"mcc_code\", \"battery\", \"device_system_version\", \"screen_size\", \"developer_tools\", \"compromised\"])\n",
    "        .sort([\"customer_id\", \"event_ts\", \"event_id\"])\n",
    "    )\n",
    "\n",
    "    # labels and sampling mask\n",
    "    lf = lf.join(labels_lf, on=\"event_id\", how=\"left\")\n",
    "    lf = lf.with_columns([\n",
    "        pl.when(pl.col(\"period\") == \"train\")\n",
    "          .then(pl.when(pl.col(\"target\").is_null()).then(pl.lit(-1)).otherwise(pl.col(\"target\")))\n",
    "          .otherwise(pl.lit(None))\n",
    "          .alias(\"train_target_raw\")\n",
    "    ])\n",
    "\n",
    "    border_expr = pl.lit(NEG_SAMPLE_BORDER_STR).str.strptime(pl.Datetime, format=\"%Y-%m-%d %H:%M:%S\", strict=False)\n",
    "    lf = lf.with_columns([\n",
    "        ((pl.col(\"period\") == \"train\") &\n",
    "         (pl.col(\"train_target_raw\") == -1) &\n",
    "         (((pl.col(\"event_ts\") >= border_expr) & ((pl.struct([\"event_id\", \"customer_id\"]).hash(seed=RANDOM_SEED) % NEG_SAMPLE_MOD_RECENT) == 0)) |\n",
    "          ((pl.col(\"event_ts\") < border_expr) & ((pl.struct([\"event_id\", \"customer_id\"]).hash(seed=RANDOM_SEED + 17) % NEG_SAMPLE_MOD_OLD) == 0))))\n",
    "          .alias(\"keep_green\")\n",
    "    ])\n",
    "    lf = lf.with_columns([\n",
    "        ((pl.col(\"period\") == \"train\") & ((pl.col(\"train_target_raw\") != -1) | pl.col(\"keep_green\"))).alias(\"is_train_sample\"),\n",
    "        (pl.col(\"period\") == \"test\").alias(\"is_test\"),\n",
    "\n",
    "        pl.col(\"event_ts\").dt.hour().cast(pl.Int8).alias(\"hour\"),\n",
    "        pl.col(\"event_ts\").dt.weekday().cast(pl.Int8).alias(\"weekday\"),\n",
    "        pl.col(\"event_ts\").dt.day().cast(pl.Int8).alias(\"day\"),\n",
    "        pl.col(\"event_ts\").dt.month().cast(pl.Int8).alias(\"month\"),\n",
    "        (pl.col(\"event_ts\").dt.weekday() >= 5).cast(pl.Int8).alias(\"is_weekend\"),\n",
    "        (pl.col(\"event_ts\").dt.epoch(\"s\") // 86400).cast(pl.Int32).alias(\"event_day_number\"),\n",
    "        pl.col(\"event_ts\").dt.date().alias(\"event_date\"),\n",
    "\n",
    "        pl.col(\"amt\").abs().log1p().cast(pl.Float32).alias(\"amt_log_abs\"),\n",
    "        (pl.col(\"amt\") < 0).cast(pl.Int8).alias(\"amt_is_negative\"),\n",
    "        (pl.col(\"screen_w\").cast(pl.Int32) * pl.col(\"screen_h\").cast(pl.Int32)).alias(\"screen_pixels\"),\n",
    "        pl.when((pl.col(\"screen_h\") > 0) & (pl.col(\"screen_w\") > 0))\n",
    "          .then(pl.col(\"screen_w\").cast(pl.Float32) / pl.col(\"screen_h\").cast(pl.Float32))\n",
    "          .otherwise(0.0)\n",
    "          .alias(\"screen_ratio\"),\n",
    "    ])\n",
    "\n",
    "    # sequential customer history features (strictly from previous events after sorting)\n",
    "    lf = lf.with_columns([\n",
    "        pl.cum_count(\"event_id\").over(\"customer_id\").cast(pl.Int32).alias(\"cust_event_idx\"),\n",
    "        pl.col(\"amt\").cum_sum().over(\"customer_id\").alias(\"cust_cum_amt\"),\n",
    "        (pl.col(\"amt\") * pl.col(\"amt\")).cum_sum().over(\"customer_id\").alias(\"cust_cum_amt_sq\"),\n",
    "        pl.col(\"event_ts\").shift(1).over(\"customer_id\").alias(\"prev_event_ts\"),\n",
    "        pl.col(\"amt\").shift(1).over(\"customer_id\").alias(\"prev_amt\"),\n",
    "\n",
    "        (pl.cum_count(\"event_id\").over([\"customer_id\", \"event_type_nm\"]) - 1).cast(pl.Int16).alias(\"cnt_prev_same_type\"),\n",
    "        (pl.cum_count(\"event_id\").over([\"customer_id\", \"event_desc\"]) - 1).cast(pl.Int16).alias(\"cnt_prev_same_desc\"),\n",
    "        (pl.cum_count(\"event_id\").over([\"customer_id\", \"mcc_code_i\"]) - 1).cast(pl.Int16).alias(\"cnt_prev_same_mcc\"),\n",
    "        (pl.cum_count(\"event_id\").over([\"customer_id\", \"channel_indicator_sub_type\"]) - 1).cast(pl.Int16).alias(\"cnt_prev_same_subtype\"),\n",
    "        (pl.cum_count(\"event_id\").over([\"customer_id\", \"session_id\"]) - 1).cast(pl.Int16).alias(\"cnt_prev_same_session\"),\n",
    "\n",
    "        pl.col(\"event_ts\").shift(1).over([\"customer_id\", \"event_type_nm\"]).alias(\"prev_same_type_ts\"),\n",
    "        pl.col(\"event_ts\").shift(1).over([\"customer_id\", \"event_desc\"]).alias(\"prev_same_desc_ts\"),\n",
    "    ])\n",
    "\n",
    "    lf = lf.with_columns([\n",
    "        (pl.col(\"cust_event_idx\") - 1).cast(pl.Int32).alias(\"cust_prev_events\"),\n",
    "        pl.when(pl.col(\"cust_event_idx\") > 1)\n",
    "          .then((pl.col(\"cust_cum_amt\") - pl.col(\"amt\")) / (pl.col(\"cust_event_idx\") - 1))\n",
    "          .otherwise(0.0)\n",
    "          .cast(pl.Float32)\n",
    "          .alias(\"cust_prev_amt_mean\"),\n",
    "        pl.when(pl.col(\"prev_event_ts\").is_not_null())\n",
    "          .then((pl.col(\"event_ts\") - pl.col(\"prev_event_ts\")).dt.total_seconds())\n",
    "          .otherwise(-1)\n",
    "          .cast(pl.Int32)\n",
    "          .alias(\"sec_since_prev_event\"),\n",
    "        (pl.col(\"amt\") - pl.col(\"prev_amt\").fill_null(0.0)).cast(pl.Float32).alias(\"amt_delta_prev\"),\n",
    "        pl.when(pl.col(\"prev_same_type_ts\").is_not_null())\n",
    "          .then((pl.col(\"event_ts\") - pl.col(\"prev_same_type_ts\")).dt.total_seconds())\n",
    "          .otherwise(-1)\n",
    "          .cast(pl.Int32)\n",
    "          .alias(\"sec_since_prev_same_type\"),\n",
    "        pl.when(pl.col(\"prev_same_desc_ts\").is_not_null())\n",
    "          .then((pl.col(\"event_ts\") - pl.col(\"prev_same_desc_ts\")).dt.total_seconds())\n",
    "          .otherwise(-1)\n",
    "          .cast(pl.Int32)\n",
    "          .alias(\"sec_since_prev_same_desc\"),\n",
    "        (pl.cum_count(\"event_id\").over([\"customer_id\", \"event_date\"]) - 1).cast(pl.Int16).alias(\"events_before_today\"),\n",
    "    ])\n",
    "\n",
    "    lf = lf.with_columns([\n",
    "        pl.when(pl.col(\"cust_event_idx\") > 2)\n",
    "          .then(\n",
    "              (\n",
    "                  ((pl.col(\"cust_cum_amt_sq\") - pl.col(\"amt\") * pl.col(\"amt\")) / (pl.col(\"cust_event_idx\") - 1))\n",
    "                  - (pl.col(\"cust_prev_amt_mean\") * pl.col(\"cust_prev_amt_mean\"))\n",
    "              )\n",
    "              .clip(lower_bound=0)\n",
    "              .sqrt()\n",
    "          )\n",
    "          .otherwise(0.0)\n",
    "          .cast(pl.Float32)\n",
    "          .alias(\"cust_prev_amt_std\")\n",
    "    ])\n",
    "\n",
    "    lf = lf.with_columns([\n",
    "        pl.when(pl.col(\"is_train_sample\")).then((pl.col(\"train_target_raw\") == 1).cast(pl.Int8)).otherwise(pl.lit(None)).alias(\"target_bin\")\n",
    "    ])\n",
    "\n",
    "    select_cols = [\"event_id\", \"period\", \"event_ts\", \"is_train_sample\", \"is_test\", \"train_target_raw\", \"target_bin\"] + FINAL_FEATURE_COLS\n",
    "\n",
    "    out_df = (\n",
    "        lf.filter(pl.col(\"is_train_sample\") | pl.col(\"is_test\"))\n",
    "          .select(select_cols)\n",
    "          .collect()\n",
    "    )\n",
    "\n",
    "    out_df.write_parquet(out_path, compression=\"zstd\")\n",
    "\n",
    "    n_train = int(out_df.filter(pl.col(\"is_train_sample\")).height)\n",
    "    n_test = int(out_df.filter(pl.col(\"is_test\")).height)\n",
    "    print(f\"[part {part_id}] done: rows={out_df.height:,}, train_sample={n_train:,}, test={n_test:,}\")\n",
    "\n",
    "    del out_df\n",
    "    gc.collect()\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Сборка фичей по частям клиентов\n",
    "Каждую часть считаем отдельно и кэшируем в `cache/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36e2e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[part 1] building features...\n",
      "[part 1] done: rows=1,743,486, train_sample=1,532,530, test=210,956\n",
      "[part 2] building features...\n",
      "[part 2] done: rows=1,742,145, train_sample=1,530,096, test=212,049\n",
      "[part 3] building features...\n",
      "[part 3] done: rows=1,736,884, train_sample=1,526,206, test=210,678\n",
      "Feature table shape: (5222515, 50)\n",
      "Train sample rows: 4588832\n",
      "Test rows: 633683\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Блок 4. Сборка и объединение признаков всех трех частей.\n",
    "# ------------------------------------------------------------\n",
    "feature_paths = []\n",
    "for part_id in [1, 2, 3]:\n",
    "    path = build_features_for_part(part_id, force=FORCE_REBUILD_FEATURES)\n",
    "    feature_paths.append(path)\n",
    "\n",
    "features = pl.concat([pl.scan_parquet(p) for p in feature_paths], how=\"vertical_relaxed\").collect()\n",
    "\n",
    "print(\"Feature table shape:\", features.shape)\n",
    "print(\"Train sample rows:\", features.filter(pl.col(\"is_train_sample\")).height)\n",
    "print(\"Test rows:\", features.filter(pl.col(\"is_test\")).height)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Category priors\n",
    "Добавляем сглаженные статистики категорий, рассчитанные только на train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf7a7614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building priors for: event_desc\n",
      "Building priors for: mcc_code_i\n",
      "Building priors for: timezone\n",
      "Building priors for: operating_system_type\n",
      "Building priors for: channel_indicator_sub_type\n",
      "Building priors for: event_type_nm\n",
      "Building priors for: pos_cd\n",
      "Feature table after priors: (5222515, 78)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Блок 5. Category priors (сглаженные статистики по train).\n",
    "# ------------------------------------------------------------\n",
    "PRIOR_COL_DEFS = {\n",
    "    \"event_desc\": pl.col(\"event_desc\").cast(pl.Int32, strict=False).fill_null(-1).alias(\"event_desc\"),\n",
    "    \"mcc_code_i\": pl.col(\"mcc_code\").cast(pl.Int32, strict=False).fill_null(-1).alias(\"mcc_code_i\"),\n",
    "    \"timezone\": pl.col(\"timezone\").cast(pl.Int32, strict=False).fill_null(-1).alias(\"timezone\"),\n",
    "    \"operating_system_type\": pl.col(\"operating_system_type\").cast(pl.Int16, strict=False).fill_null(-1).alias(\"operating_system_type\"),\n",
    "    \"channel_indicator_sub_type\": pl.col(\"channel_indicator_sub_type\").cast(pl.Int16, strict=False).fill_null(-1).alias(\"channel_indicator_sub_type\"),\n",
    "    \"event_type_nm\": pl.col(\"event_type_nm\").cast(pl.Int32, strict=False).fill_null(-1).alias(\"event_type_nm\"),\n",
    "    \"pos_cd\": pl.col(\"pos_cd\").cast(pl.Int16, strict=False).fill_null(-1).alias(\"pos_cd\"),\n",
    "}\n",
    "\n",
    "\n",
    "def _train_scan_with_expr(expr: pl.Expr, key_name: str) -> pl.LazyFrame:\n",
    "    return pl.concat([\n",
    "        pl.scan_parquet(DATA_DIR / f\"train_part_{i}.parquet\")\n",
    "          .select([pl.col(\"event_id\"), expr])\n",
    "        for i in [1, 2, 3]\n",
    "    ], how=\"vertical_relaxed\")\n",
    "\n",
    "\n",
    "def build_prior_table(key_name: str, expr: pl.Expr, force: bool = False) -> pl.DataFrame:\n",
    "    out_path = CACHE_DIR / f\"prior_{key_name}.parquet\"\n",
    "    if out_path.exists() and (not force):\n",
    "        return pl.read_parquet(out_path)\n",
    "\n",
    "    print(f\"Building priors for: {key_name}\")\n",
    "    lf = _train_scan_with_expr(expr, key_name)\n",
    "\n",
    "    cnt_col = f\"prior_{key_name}_cnt\"\n",
    "    lbl_cnt_col = f\"prior_{key_name}_lbl_cnt\"\n",
    "    red_cnt_col = f\"prior_{key_name}_red_cnt\"\n",
    "\n",
    "    total = lf.group_by(key_name).len().rename({\"len\": cnt_col})\n",
    "    labeled = (\n",
    "        lf.join(labels_lf, on=\"event_id\", how=\"inner\")\n",
    "          .group_by(key_name)\n",
    "          .agg([\n",
    "              pl.len().alias(lbl_cnt_col),\n",
    "              pl.sum(\"target\").cast(pl.Float64).alias(red_cnt_col),\n",
    "          ])\n",
    "    )\n",
    "\n",
    "    prior = (\n",
    "        total.join(labeled, on=key_name, how=\"left\")\n",
    "             .with_columns([\n",
    "                 pl.col(lbl_cnt_col).fill_null(0.0),\n",
    "                 pl.col(red_cnt_col).fill_null(0.0),\n",
    "             ])\n",
    "             .with_columns([\n",
    "                 ((pl.col(red_cnt_col) + 1.0) / (pl.col(cnt_col) + 200.0)).cast(pl.Float32).alias(f\"prior_{key_name}_red_rate_all\"),\n",
    "                 ((pl.col(lbl_cnt_col) + 1.0) / (pl.col(cnt_col) + 200.0)).cast(pl.Float32).alias(f\"prior_{key_name}_labeled_rate_all\"),\n",
    "                 ((pl.col(red_cnt_col) + 1.0) / (pl.col(lbl_cnt_col) + 2.0)).cast(pl.Float32).alias(f\"prior_{key_name}_red_share_labeled\"),\n",
    "             ])\n",
    "             .select([\n",
    "                 key_name,\n",
    "                 cnt_col,\n",
    "                 f\"prior_{key_name}_red_rate_all\",\n",
    "                 f\"prior_{key_name}_labeled_rate_all\",\n",
    "                 f\"prior_{key_name}_red_share_labeled\",\n",
    "             ])\n",
    "             .collect()\n",
    "    )\n",
    "\n",
    "    prior.write_parquet(out_path, compression=\"zstd\")\n",
    "    return prior\n",
    "\n",
    "\n",
    "prior_feature_cols = []\n",
    "if ADD_CATEGORY_PRIORS:\n",
    "    for key_name, expr in PRIOR_COL_DEFS.items():\n",
    "        prior_df = build_prior_table(key_name, expr, force=FORCE_REBUILD_PRIORS)\n",
    "        features = features.join(prior_df, on=key_name, how=\"left\")\n",
    "        prior_feature_cols.extend([c for c in prior_df.columns if c != key_name])\n",
    "\n",
    "    fill_exprs = [pl.col(c).fill_null(pl.col(c).mean()).alias(c) for c in prior_feature_cols]\n",
    "    if fill_exprs:\n",
    "        features = features.with_columns(fill_exprs)\n",
    "\n",
    "print(\"Feature table after priors:\", features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Подготовка train/test для модели\n",
    "Переходим в `pandas`, заполняем пропуски, делаем time-ordered holdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aceb5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample: (4588832, 78)\n",
      "Test rows: (633683, 78)\n",
      "Features: 71\n",
      "Categorical features: 14\n",
      "Numerical features: 57\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Блок 6. Подготовка таблиц для CatBoost и контроль leakage.\n",
    "# ------------------------------------------------------------\n",
    "train_pl = features.filter(pl.col(\"is_train_sample\")).with_columns([\n",
    "    pl.col(\"target_bin\").cast(pl.Int8),\n",
    "])\n",
    "test_pl = features.filter(pl.col(\"is_test\"))\n",
    "\n",
    "print(\"Train sample:\", train_pl.shape)\n",
    "print(\"Test rows:\", test_pl.shape)\n",
    "\n",
    "train_df = train_pl.to_pandas()\n",
    "test_df = test_pl.to_pandas()\n",
    "\n",
    "del features, train_pl, test_pl\n",
    "gc.collect()\n",
    "\n",
    "train_df[\"event_ts\"] = pd.to_datetime(train_df[\"event_ts\"])\n",
    "test_df[\"event_ts\"] = pd.to_datetime(test_df[\"event_ts\"])\n",
    "\n",
    "feature_cols = [c for c in train_df.columns if c not in META_COLS]\n",
    "if ADD_CATEGORY_PRIORS:\n",
    "    feature_cols = [c for c in feature_cols if c != \"target\"]\n",
    "\n",
    "# ensure no leakage columns in features\n",
    "for bad_col in [\"target\", \"keep_green\", \"event_date\"]:\n",
    "    if bad_col in feature_cols:\n",
    "        feature_cols.remove(bad_col)\n",
    "\n",
    "cat_cols = [c for c in CAT_COLS if c in feature_cols]\n",
    "num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "for c in cat_cols:\n",
    "    train_df[c] = train_df[c].fillna(-1).astype(np.int64)\n",
    "    test_df[c] = test_df[c].fillna(-1).astype(np.int64)\n",
    "\n",
    "# robust fill for numeric columns\n",
    "medians = train_df[num_cols].median(numeric_only=True)\n",
    "train_df[num_cols] = train_df[num_cols].fillna(medians)\n",
    "test_df[num_cols] = test_df[num_cols].fillna(medians)\n",
    "\n",
    "# keep chronological order for validation split\n",
    "train_df = train_df.sort_values(\"event_ts\").reset_index(drop=True)\n",
    "\n",
    "print(\"Features:\", len(feature_cols))\n",
    "print(\"Categorical features:\", len(cat_cols))\n",
    "print(\"Numerical features:\", len(num_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Утилиты обучения\n",
    "Функции весов, обучения с holdout и переобучения на полном train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cb01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Блок 7. Функции для обучения CatBoost и рефита на full train.\n",
    "# ------------------------------------------------------------\n",
    "def make_weights(raw_target: np.ndarray) -> np.ndarray:\n",
    "    # raw_target: 1=red, 0=yellow, -1=green sampled\n",
    "    return np.where(raw_target == 1, 10.0, np.where(raw_target == 0, 2.5, 1.0)).astype(np.float32)\n",
    "\n",
    "\n",
    "def fit_catboost_with_holdout(X_tr, y_tr, w_tr, X_val, y_val, w_val, cat_cols, params, use_gpu=True):\n",
    "    params = params.copy()\n",
    "    params.update({\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"allow_writing_files\": False,\n",
    "        \"verbose\": 200,\n",
    "        \"metric_period\": 100,\n",
    "    })\n",
    "\n",
    "    if use_gpu:\n",
    "        params.update({\"task_type\": \"GPU\", \"devices\": \"0\"})\n",
    "    else:\n",
    "        params.update({\"task_type\": \"CPU\", \"thread_count\": max(1, (os.cpu_count() or 4) - 1)})\n",
    "\n",
    "    train_pool = Pool(X_tr, y_tr, weight=w_tr, cat_features=cat_cols)\n",
    "    val_pool = Pool(X_val, y_val, weight=w_val, cat_features=cat_cols)\n",
    "\n",
    "    try:\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "    except Exception as e:\n",
    "        print(\"GPU fit failed, fallback to CPU:\", e)\n",
    "        params.pop(\"devices\", None)\n",
    "        params[\"task_type\"] = \"CPU\"\n",
    "        params[\"thread_count\"] = max(1, (os.cpu_count() or 4) - 1)\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "\n",
    "    val_raw = model.predict(val_pool, prediction_type=\"RawFormulaVal\")\n",
    "    val_ap = average_precision_score(y_val, val_raw)\n",
    "    best_iter = model.get_best_iteration()\n",
    "    if best_iter is None or best_iter <= 0:\n",
    "        best_iter = params.get(\"iterations\", 1000)\n",
    "\n",
    "    print(f\"best_iter={best_iter}, val_pr_auc={val_ap:.6f}\")\n",
    "    return model, best_iter, val_ap, params\n",
    "\n",
    "\n",
    "def refit_full_catboost(X, y, w, cat_cols, base_params, best_iter):\n",
    "    params = base_params.copy()\n",
    "    params.pop(\"od_type\", None)\n",
    "    params.pop(\"od_wait\", None)\n",
    "    params[\"iterations\"] = int(max(300, best_iter))\n",
    "\n",
    "    y_arr = np.asarray(y)\n",
    "    w_arr = np.asarray(w, dtype=np.float32)\n",
    "\n",
    "    # Safety: guard against scalar or stale/mismatched weights from notebook state\n",
    "    if w_arr.ndim == 0:\n",
    "        w_arr = np.full(shape=(len(y_arr),), fill_value=float(w_arr), dtype=np.float32)\n",
    "    elif w_arr.shape[0] != len(y_arr):\n",
    "        fill_value = float(np.nanmean(w_arr)) if w_arr.size > 0 else 1.0\n",
    "        if not np.isfinite(fill_value):\n",
    "            fill_value = 1.0\n",
    "        w_arr = np.full(shape=(len(y_arr),), fill_value=fill_value, dtype=np.float32)\n",
    "\n",
    "    pool = Pool(X, y_arr, weight=w_arr, cat_features=cat_cols)\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(pool, verbose=200)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Main-модель\n",
    "Базовый red-vs-all классификатор на полном train-сэмпле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6961d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main train rows: 3317752 Main val rows: 1271080\n",
      "0:\ttest: 0.7249056\tbest: 0.7249056 (0)\ttotal: 397ms\tremaining: 33m 3s\n",
      "200:\ttest: 0.8366974\tbest: 0.8367093 (199)\ttotal: 29.4s\tremaining: 11m 41s\n",
      "400:\ttest: 0.8444534\tbest: 0.8450442 (354)\ttotal: 58.3s\tremaining: 11m 8s\n",
      "600:\ttest: 0.8421774\tbest: 0.8450442 (354)\ttotal: 1m 27s\tremaining: 10m 42s\n",
      "bestTest = 0.8450441957\n",
      "bestIteration = 354\n",
      "Shrink model to first 355 iterations.\n",
      "best_iter=354, val_pr_auc=0.071459\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Блок 8. Обучение MAIN-модели и holdout-валидация.\n",
    "# ------------------------------------------------------------\n",
    "val_mask = train_df[\"event_ts\"] >= VAL_START\n",
    "\n",
    "# MAIN MODEL: full sample timeline\n",
    "X_main = train_df[feature_cols]\n",
    "y_main = train_df[\"target_bin\"].astype(np.int8).values\n",
    "w_main = make_weights(train_df[\"train_target_raw\"].values)\n",
    "\n",
    "X_main_tr = X_main.loc[~val_mask]\n",
    "y_main_tr = y_main[~val_mask]\n",
    "w_main_tr = w_main[~val_mask]\n",
    "\n",
    "X_main_val = X_main.loc[val_mask]\n",
    "y_main_val = y_main[val_mask]\n",
    "w_main_val = w_main[val_mask]\n",
    "\n",
    "print(\"Main train rows:\", len(X_main_tr), \"Main val rows:\", len(X_main_val))\n",
    "\n",
    "params_main = {\n",
    "    \"iterations\": 5000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 8,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 300,\n",
    "}\n",
    "\n",
    "model_main, best_iter_main, ap_main, used_params_main = fit_catboost_with_holdout(\n",
    "    X_main_tr, y_main_tr, w_main_tr,\n",
    "    X_main_val, y_main_val, w_main_val,\n",
    "    cat_cols=cat_cols,\n",
    "    params=params_main,\n",
    "    use_gpu=USE_GPU,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Recent-модель и промежуточный бленд\n",
    "Фокус на свежем режиме + все размеченные события."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a48f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent train rows: 1962395 Recent val rows: 1271080\n",
      "0:\ttest: 0.7099752\tbest: 0.7099752 (0)\ttotal: 338ms\tremaining: 28m 11s\n",
      "200:\ttest: 0.8395559\tbest: 0.8396364 (198)\ttotal: 20.1s\tremaining: 8m\n",
      "400:\ttest: 0.8411693\tbest: 0.8417438 (275)\ttotal: 39.9s\tremaining: 7m 37s\n",
      "bestTest = 0.8417437971\n",
      "bestIteration = 275\n",
      "Shrink model to first 276 iterations.\n",
      "best_iter=275, val_pr_auc=0.067203\n",
      "Main val PR-AUC:   0.071459\n",
      "Recent val PR-AUC: 0.067203\n",
      "Blend val PR-AUC:  0.072120\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Блок 9. RECENT-модель и первичный blend main+recent.\n",
    "# ------------------------------------------------------------\n",
    "# RECENT MODEL: stronger focus on latest regime + all labeled events\n",
    "recent_mask = (train_df[\"event_ts\"] >= RECENT_BORDER) | (train_df[\"train_target_raw\"] != -1)\n",
    "recent_train_mask = recent_mask & (~val_mask)\n",
    "recent_val_mask = recent_mask & val_mask\n",
    "\n",
    "X_recent = train_df.loc[recent_mask, feature_cols]\n",
    "y_recent = train_df.loc[recent_mask, \"target_bin\"].astype(np.int8).values\n",
    "w_recent = make_weights(train_df.loc[recent_mask, \"train_target_raw\"].values)\n",
    "\n",
    "X_recent_tr = train_df.loc[recent_train_mask, feature_cols]\n",
    "y_recent_tr = train_df.loc[recent_train_mask, \"target_bin\"].astype(np.int8).values\n",
    "w_recent_tr = make_weights(train_df.loc[recent_train_mask, \"train_target_raw\"].values)\n",
    "\n",
    "X_recent_val = train_df.loc[recent_val_mask, feature_cols]\n",
    "y_recent_val = train_df.loc[recent_val_mask, \"target_bin\"].astype(np.int8).values\n",
    "w_recent_val = make_weights(train_df.loc[recent_val_mask, \"train_target_raw\"].values)\n",
    "\n",
    "print(\"Recent train rows:\", len(X_recent_tr), \"Recent val rows:\", len(X_recent_val))\n",
    "\n",
    "params_recent = {\n",
    "    \"iterations\": 5000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 8,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 300,\n",
    "}\n",
    "\n",
    "model_recent, best_iter_recent, ap_recent, used_params_recent = fit_catboost_with_holdout(\n",
    "    X_recent_tr, y_recent_tr, w_recent_tr,\n",
    "    X_recent_val, y_recent_val, w_recent_val,\n",
    "    cat_cols=cat_cols,\n",
    "    params=params_recent,\n",
    "    use_gpu=USE_GPU,\n",
    ")\n",
    "\n",
    "# blended holdout score\n",
    "val_pool_main = Pool(X_main_val, y_main_val, cat_features=cat_cols)\n",
    "val_pool_recent = Pool(X_recent_val, y_recent_val, cat_features=cat_cols)\n",
    "\n",
    "pred_main_val = model_main.predict(val_pool_main, prediction_type=\"RawFormulaVal\")\n",
    "pred_recent_val = model_recent.predict(val_pool_recent, prediction_type=\"RawFormulaVal\")\n",
    "\n",
    "# align recent val predictions to global val index\n",
    "val_index = train_df.index[val_mask]\n",
    "recent_val_index = train_df.index[recent_val_mask]\n",
    "recent_pred_map = pd.Series(pred_recent_val, index=recent_val_index)\n",
    "recent_pred_aligned = recent_pred_map.reindex(val_index).fillna(recent_pred_map.mean()).values\n",
    "\n",
    "blend_val = 0.7 * pred_main_val + 0.3 * recent_pred_aligned\n",
    "blend_ap = average_precision_score(y_main_val, blend_val)\n",
    "\n",
    "print(f\"Main val PR-AUC:   {ap_main:.6f}\")\n",
    "print(f\"Recent val PR-AUC: {ap_recent:.6f}\")\n",
    "print(f\"Blend val PR-AUC:  {blend_ap:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Расширенный ансамбль и сабмит\n",
    "Две дополнительные модели, подбор весов, финальный `submission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d8a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7410243\tbest: 0.7410243 (0)\ttotal: 342ms\tremaining: 28m 30s\n",
      "200:\ttest: 0.8071643\tbest: 0.8071643 (200)\ttotal: 29.6s\tremaining: 11m 45s\n",
      "400:\ttest: 0.8152986\tbest: 0.8152986 (400)\ttotal: 58.5s\tremaining: 11m 11s\n",
      "600:\ttest: 0.8183179\tbest: 0.8184517 (585)\ttotal: 1m 27s\tremaining: 10m 41s\n",
      "800:\ttest: 0.8195700\tbest: 0.8196511 (790)\ttotal: 1m 56s\tremaining: 10m 10s\n",
      "1000:\ttest: 0.8212683\tbest: 0.8212683 (1000)\ttotal: 2m 26s\tremaining: 9m 44s\n",
      "1200:\ttest: 0.8213539\tbest: 0.8214045 (1191)\ttotal: 2m 55s\tremaining: 9m 15s\n",
      "1400:\ttest: 0.8217282\tbest: 0.8217632 (1388)\ttotal: 3m 25s\tremaining: 8m 47s\n",
      "1600:\ttest: 0.8218916\tbest: 0.8219096 (1532)\ttotal: 4m 5s\tremaining: 8m 41s\n",
      "1800:\ttest: 0.8216233\tbest: 0.8219729 (1628)\ttotal: 4m 42s\tremaining: 8m 21s\n",
      "bestTest = 0.8219728768\n",
      "bestIteration = 1628\n",
      "Shrink model to first 1629 iterations.\n",
      "best_iter=1628, val_pr_auc=0.117400\n",
      "0:\ttest: 0.7282351\tbest: 0.7282351 (0)\ttotal: 58.8ms\tremaining: 4m 53s\n",
      "200:\ttest: 0.7424495\tbest: 0.7644155 (5)\ttotal: 11.5s\tremaining: 4m 33s\n",
      "bestTest = 0.7644155025\n",
      "bestIteration = 5\n",
      "Shrink model to first 6 iterations.\n",
      "best_iter=5, val_pr_auc=0.779670\n",
      "Main val PR-AUC:   0.071459\n",
      "Recent val PR-AUC: 0.067203\n",
      "Prod val PR-AUC:   0.088557\n",
      "Best blend weights (main,recent,prod): (0.3, 0.0, 0.7)\n",
      "Best blended val PR-AUC: 0.096542\n",
      "Refit full MAIN model...\n",
      "0:\ttotal: 400ms\tremaining: 2m 21s\n",
      "200:\ttotal: 40.7s\tremaining: 31s\n",
      "353:\ttotal: 1m 10s\tremaining: 0us\n",
      "Refit full RECENT model...\n",
      "0:\ttotal: 408ms\tremaining: 2m 2s\n",
      "200:\ttotal: 27.8s\tremaining: 13.7s\n",
      "299:\ttotal: 41.4s\tremaining: 0us\n",
      "Refit full SUSP model...\n",
      "0:\ttotal: 450ms\tremaining: 12m 11s\n",
      "200:\ttotal: 44.3s\tremaining: 5m 14s\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Блок 10. Дополнительные модели, подбор blend и submission.\n",
    "# ------------------------------------------------------------\n",
    "# Extra models: suspicious and red|suspicious + better blend\n",
    "import numpy as np\n",
    "\n",
    "def _sigmoid(x):\n",
    "    x = np.clip(x, -40, 40)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def _logit(p):\n",
    "    p = np.clip(p, 1e-8, 1 - 1e-8)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "raw_target = train_df[\"train_target_raw\"].values\n",
    "X_all = train_df[feature_cols]\n",
    "\n",
    "# 1) suspicious model: (red + yellow) vs green\n",
    "y_susp = (raw_target != -1).astype(np.int8)\n",
    "w_susp = np.where(raw_target != -1, 6.0, 1.2).astype(np.float32)\n",
    "\n",
    "X_susp_tr = X_all.loc[~val_mask]\n",
    "y_susp_tr = y_susp[~val_mask]\n",
    "w_susp_tr = w_susp[~val_mask]\n",
    "\n",
    "X_susp_val = X_all.loc[val_mask]\n",
    "y_susp_val = y_susp[val_mask]\n",
    "w_susp_val = w_susp[val_mask]\n",
    "\n",
    "params_susp = {\n",
    "    \"iterations\": 5000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 8,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 300,\n",
    "}\n",
    "\n",
    "model_susp, best_iter_susp, ap_susp, used_params_susp = fit_catboost_with_holdout(\n",
    "    X_susp_tr, y_susp_tr, w_susp_tr,\n",
    "    X_susp_val, y_susp_val, w_susp_val,\n",
    "    cat_cols=cat_cols,\n",
    "    params=params_susp,\n",
    "    use_gpu=USE_GPU,\n",
    ")\n",
    "\n",
    "# 2) red|suspicious model: red vs yellow on labeled only\n",
    "labeled_mask = train_df[\"train_target_raw\"].values != -1\n",
    "labeled_train_mask = labeled_mask & (~val_mask)\n",
    "labeled_val_mask = labeled_mask & val_mask\n",
    "\n",
    "y_rg = train_df.loc[labeled_mask, \"target_bin\"].astype(np.int8).values\n",
    "w_rg = np.where(train_df.loc[labeled_mask, \"train_target_raw\"].values == 1, 2.2, 1.0).astype(np.float32)\n",
    "\n",
    "X_rg_tr = train_df.loc[labeled_train_mask, feature_cols]\n",
    "y_rg_tr = train_df.loc[labeled_train_mask, \"target_bin\"].astype(np.int8).values\n",
    "w_rg_tr = np.where(train_df.loc[labeled_train_mask, \"train_target_raw\"].values == 1, 2.2, 1.0).astype(np.float32)\n",
    "\n",
    "X_rg_val = train_df.loc[labeled_val_mask, feature_cols]\n",
    "y_rg_val = train_df.loc[labeled_val_mask, \"target_bin\"].astype(np.int8).values\n",
    "w_rg_val = np.where(train_df.loc[labeled_val_mask, \"train_target_raw\"].values == 1, 2.2, 1.0).astype(np.float32)\n",
    "\n",
    "params_rg = {\n",
    "    \"iterations\": 5000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 8,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 300,\n",
    "}\n",
    "\n",
    "model_rg, best_iter_rg, ap_rg, used_params_rg = fit_catboost_with_holdout(\n",
    "    X_rg_tr, y_rg_tr, w_rg_tr,\n",
    "    X_rg_val, y_rg_val, w_rg_val,\n",
    "    cat_cols=cat_cols,\n",
    "    params=params_rg,\n",
    "    use_gpu=USE_GPU,\n",
    ")\n",
    "\n",
    "# Holdout predictions (global validation)\n",
    "val_pool = Pool(X_main_val, y_main_val, cat_features=cat_cols)\n",
    "\n",
    "pred_main_val = model_main.predict(val_pool, prediction_type=\"RawFormulaVal\")\n",
    "pred_recent_val = model_recent.predict(val_pool, prediction_type=\"RawFormulaVal\")\n",
    "pred_susp_val = model_susp.predict(val_pool, prediction_type=\"RawFormulaVal\")\n",
    "pred_rg_val = model_rg.predict(val_pool, prediction_type=\"RawFormulaVal\")\n",
    "pred_prod_val = _logit(_sigmoid(pred_susp_val) * _sigmoid(pred_rg_val))\n",
    "\n",
    "best_ap = -1.0\n",
    "best_w = None\n",
    "for blend_w_main in np.arange(0.30, 0.91, 0.05):\n",
    "    for blend_w_recent in np.arange(0.00, 0.41, 0.05):\n",
    "        blend_w_prod = 1.0 - blend_w_main - blend_w_recent\n",
    "        if blend_w_prod < 0:\n",
    "            continue\n",
    "        blend = blend_w_main * pred_main_val + blend_w_recent * pred_recent_val + blend_w_prod * pred_prod_val\n",
    "        ap = average_precision_score(y_main_val, blend)\n",
    "        if ap > best_ap:\n",
    "            best_ap = ap\n",
    "            best_w = (float(blend_w_main), float(blend_w_recent), float(blend_w_prod))\n",
    "\n",
    "print(f\"Main val PR-AUC:   {average_precision_score(y_main_val, pred_main_val):.6f}\")\n",
    "print(f\"Recent val PR-AUC: {average_precision_score(y_main_val, pred_recent_val):.6f}\")\n",
    "print(f\"Prod val PR-AUC:   {average_precision_score(y_main_val, pred_prod_val):.6f}\")\n",
    "print(\"Best blend weights (main,recent,prod):\", best_w)\n",
    "print(f\"Best blended val PR-AUC: {best_ap:.6f}\")\n",
    "\n",
    "# Refit and test prediction\n",
    "# Recompute robust weight arrays locally to avoid stale notebook state side-effects\n",
    "w_main_full = make_weights(train_df[\"train_target_raw\"].values)\n",
    "w_recent_full = make_weights(train_df.loc[recent_mask, \"train_target_raw\"].values)\n",
    "\n",
    "if RETRAIN_ON_FULL:\n",
    "    print(\"Refit full MAIN model...\")\n",
    "    model_main_final = refit_full_catboost(\n",
    "        X_main, y_main, w_main_full,\n",
    "        cat_cols=cat_cols,\n",
    "        base_params=used_params_main,\n",
    "        best_iter=best_iter_main,\n",
    "    )\n",
    "\n",
    "    print(\"Refit full RECENT model...\")\n",
    "    model_recent_final = refit_full_catboost(\n",
    "        X_recent, y_recent, w_recent_full,\n",
    "        cat_cols=cat_cols,\n",
    "        base_params=used_params_recent,\n",
    "        best_iter=best_iter_recent,\n",
    "    )\n",
    "\n",
    "    print(\"Refit full SUSP model...\")\n",
    "    model_susp_final = refit_full_catboost(\n",
    "        X_all, y_susp, w_susp,\n",
    "        cat_cols=cat_cols,\n",
    "        base_params=used_params_susp,\n",
    "        best_iter=best_iter_susp,\n",
    "    )\n",
    "\n",
    "    print(\"Refit full RED|SUSP model...\")\n",
    "    model_rg_final = refit_full_catboost(\n",
    "        train_df.loc[labeled_mask, feature_cols], y_rg, w_rg,\n",
    "        cat_cols=cat_cols,\n",
    "        base_params=used_params_rg,\n",
    "        best_iter=best_iter_rg,\n",
    "    )\n",
    "else:\n",
    "    model_main_final = model_main\n",
    "    model_recent_final = model_recent\n",
    "    model_susp_final = model_susp\n",
    "    model_rg_final = model_rg\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "test_pool = Pool(X_test, cat_features=cat_cols)\n",
    "\n",
    "test_pred_main = model_main_final.predict(test_pool, prediction_type=\"RawFormulaVal\")\n",
    "test_pred_recent = model_recent_final.predict(test_pool, prediction_type=\"RawFormulaVal\")\n",
    "test_pred_susp = model_susp_final.predict(test_pool, prediction_type=\"RawFormulaVal\")\n",
    "test_pred_rg = model_rg_final.predict(test_pool, prediction_type=\"RawFormulaVal\")\n",
    "test_pred_prod = _logit(_sigmoid(test_pred_susp) * _sigmoid(test_pred_rg))\n",
    "\n",
    "w_m, w_r, w_p = best_w\n",
    "test_pred_blend = w_m * test_pred_main + w_r * test_pred_recent + w_p * test_pred_prod\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"event_id\": test_df[\"event_id\"].values,\n",
    "    \"predict\": test_pred_blend,\n",
    "})\n",
    "\n",
    "sample_submit = pd.read_csv(DATA_DIR / \"sample_submit.csv\")\n",
    "submission = sample_submit[[\"event_id\"]].merge(pred_df, on=\"event_id\", how=\"left\")\n",
    "\n",
    "missing = submission[\"predict\"].isna().sum()\n",
    "print(\"Submission rows:\", len(submission), \"Missing predictions:\", int(missing))\n",
    "assert missing == 0, \"Some test event_id are missing in predictions\"\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved -> submission.csv\")\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дальнейшие улучшения\n",
    "\n",
    "1. Добавить rolling time-CV по неделям вместо одного holdout.\n",
    "2. Подобрать веса классов и `NEG_SAMPLE_MOD_*` через небольшой search.\n",
    "3. Вынести blend в meta-model по holdout-прогнозам.\n",
    "4. Добавить взаимодействия категорий для priors (например, `event_type_nm × mcc_code`).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
